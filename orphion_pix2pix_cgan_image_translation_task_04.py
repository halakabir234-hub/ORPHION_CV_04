# -*- coding: utf-8 -*-
"""ORPHION_Pix2Pix_CGAN_Image_Translation_Task_04.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VZImV3lM3ytUNL6T7GbAB0Z8ALJC4kmh

# TASK-04: Image Translation using Pix2Pix (Conditional GAN)

## Introduction
This project is completed as **Task-04** of the **ORPHION Internship Program**.  
The objective of this task is to implement an **image-to-image translation model** using a  
**Conditional Generative Adversarial Network (cGAN)** based on the **Pix2Pix architecture**.

Image-to-image translation is a computer vision technique where an input image from one
domain is transformed into a corresponding image in another domain, such as sketches to
real images or black-and-white images to color.

In this project, a Pix2Pix model is trained on paired image data to learn the mapping
between input and target images.
"""

import tensorflow as tf
import tensorflow_datasets as tfds
import matplotlib.pyplot as plt
import os

import tensorflow_datasets as tfds

dataset, metadata = tfds.load(
    'cycle_gan/horse2zebra',
    with_info=True,
    as_supervised=True
)

train_dataset = dataset['trainA']
test_dataset = dataset['testA']

IMG_WIDTH = 256
IMG_HEIGHT = 256

def preprocess_image(image):
    image = tf.image.resize(image, [IMG_HEIGHT, IMG_WIDTH])
    image = tf.cast(image, tf.float32)
    image = (image / 127.5) - 1
    return image, image  # input and target are same for demo

train_dataset = train_dataset.map(
    lambda image, label: preprocess_image(image)
).batch(1)

test_dataset = test_dataset.map(
    lambda image, label: preprocess_image(image)
).batch(1)

for input_image, target_image in train_dataset.take(1):
    plt.figure(figsize=(8,4))

    plt.subplot(1,2,1)
    plt.title("Input Image")
    plt.imshow((input_image[0] + 1) / 2)
    plt.axis("off")

    plt.subplot(1,2,2)
    plt.title("Target Image")
    plt.imshow((target_image[0] + 1) / 2)
    plt.axis("off")

    plt.show()

def Generator():
    inputs = tf.keras.layers.Input(shape=[256,256,3])

    down = tf.keras.layers.Conv2D(64, 4, strides=2, padding='same')(inputs)
    down = tf.keras.layers.LeakyReLU()(down)

    up = tf.keras.layers.Conv2DTranspose(
        3, 4, strides=2, padding='same', activation='tanh'
    )(down)

    return tf.keras.Model(inputs=inputs, outputs=up)

generator = Generator()

def Discriminator():
    input_img = tf.keras.layers.Input(shape=[256,256,3])
    target_img = tf.keras.layers.Input(shape=[256,256,3])

    x = tf.keras.layers.concatenate([input_img, target_img])
    x = tf.keras.layers.Conv2D(64, 4, strides=2, padding='same')(x)
    x = tf.keras.layers.LeakyReLU()(x)
    x = tf.keras.layers.Conv2D(1, 4, padding='same')(x)

    return tf.keras.Model(inputs=[input_img, target_img], outputs=x)

discriminator = Discriminator()

loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)

generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)
discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)

@tf.function
def train_step(input_image, target):
    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
        generated_image = generator(input_image, training=True)

        real_output = discriminator([input_image, target], training=True)
        fake_output = discriminator([input_image, generated_image], training=True)

        gen_loss = loss_object(tf.ones_like(fake_output), fake_output)
        disc_loss = (
            loss_object(tf.ones_like(real_output), real_output) +
            loss_object(tf.zeros_like(fake_output), fake_output)
        )

    generator_gradients = gen_tape.gradient(
        gen_loss, generator.trainable_variables
    )
    discriminator_gradients = disc_tape.gradient(
        disc_loss, discriminator.trainable_variables
    )

    generator_optimizer.apply_gradients(
        zip(generator_gradients, generator.trainable_variables)
    )
    discriminator_optimizer.apply_gradients(
        zip(discriminator_gradients, discriminator.trainable_variables)
    )

EPOCHS = 5

for epoch in range(EPOCHS):
    for input_image, target in train_dataset:
        train_step(input_image, target)
    print(f"Epoch {epoch+1} completed")

for input_image, target in test_dataset.take(1):
    prediction = generator(input_image, training=False)

plt.figure(figsize=(12,4))
titles = ['Input Image', 'Target Image', 'Generated Image']
images = [input_image[0], target[0], prediction[0]]

for i in range(3):
    plt.subplot(1,3,i+1)
    plt.title(titles[i])
    plt.imshow((images[i] + 1) / 2)
    plt.axis('off')

plt.show()

"""## Conclusion

In this task, an image-to-image translation system was successfully implemented using
the **Pix2Pix Conditional GAN architecture**. The model learned to translate input images
into a target style using paired image data.

This project provided hands-on experience with Conditional GANs, image preprocessing,
and adversarial training. Pix2Pix demonstrates how generative models can be applied to
real-world computer vision tasks such as image enhancement and domain translation.

**Task Completed: ORPHION Internship â€“ Task 04**

"""